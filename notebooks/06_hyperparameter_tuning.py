# ===================================
# HYPERPARAMETER TUNING - COMPLETE OPTIMIZATION
# Methods: GridSearchCV & RandomizedSearchCV
# ===================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import (GridSearchCV, RandomizedSearchCV, 
                                   train_test_split, cross_val_score, StratifiedKFold)
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                           classification_report, confusion_matrix, roc_auc_score)
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pickle
import time
from scipy.stats import randint, uniform
import warnings
warnings.filterwarnings('ignore')

print("âš¡ HYPERPARAMETER TUNING - COMPLETE OPTIMIZATION")
print("="*80)

# ===================================
# STEP 1: LOAD DATA AND BASELINE MODELS
# ===================================
print("\nðŸ“¥ STEP 1: LOADING DATA AND BASELINE MODELS")
print("="*60)

try:
    # Load the selected features and target
    X_selected = pd.read_csv('data/X_selected_features.csv')
    y_processed = pd.read_csv('data/y_processed.csv')
    
    # Convert target to 1D array if needed
    if y_processed.shape[1] == 1:
        y = y_processed.iloc[:, 0].values
    else:
        y = y_processed.values.ravel()
    
    print("âœ… Data loaded successfully!")
    print(f"ðŸ“Š Features shape: {X_selected.shape}")
    print(f"ðŸŽ¯ Target shape: {y.shape}")
    print(f"ðŸ“‹ Target classes: {np.unique(y)}")
    
    # Load baseline model performance for comparison
    try:
        import json
        with open('results/detailed_model_results.json', 'r') as f:
            baseline_results = json.load(f)
        print("âœ… Baseline model results loaded for comparison")
    except:
        print("âš ï¸ Baseline results not found - will create comparison anyway")
        baseline_results = {}
    
except Exception as e:
    print(f"âŒ Error loading data: {e}")
    print("ðŸ’¡ Make sure you've run previous steps first!")
    exit()

# Split data for tuning
X_train, X_test, y_train, y_test = train_test_split(
    X_selected, y, test_size=0.20, random_state=42, stratify=y
)

print(f"ðŸ“Š Train/Test split: {X_train.shape[0]}/{X_test.shape[0]} samples")

# ===================================
# STEP 2: DEFINE HYPERPARAMETER GRIDS
# ===================================
print("\nðŸ”§ STEP 2: DEFINING HYPERPARAMETER GRIDS")
print("="*60)

# Define parameter grids for each model
param_grids = {
    'Logistic Regression': {
        'grid': {
            'C': [0.01, 0.1, 1, 10, 100],
            'solver': ['liblinear', 'lbfgs'],
            'penalty': ['l2'],
            'max_iter': [1000, 2000]
        },
        'random': {
            'C': uniform(0.01, 100),
            'solver': ['liblinear', 'lbfgs'],
            'penalty': ['l2'],
            'max_iter': [1000, 2000, 3000]
        }
    },
    
    'Decision Tree': {
        'grid': {
            'max_depth': [3, 5, 10, 15, 20, None],
            'min_samples_split': [2, 5, 10, 20],
            'min_samples_leaf': [1, 2, 5, 10],
            'criterion': ['gini', 'entropy']
        },
        'random': {
            'max_depth': [3, 5, 10, 15, 20, 25, None],
            'min_samples_split': randint(2, 50),
            'min_samples_leaf': randint(1, 20),
            'criterion': ['gini', 'entropy'],
            'max_features': [None, 'sqrt', 'log2']
        }
    },
    
    'Random Forest': {
        'grid': {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 10, 15, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 5],
            'max_features': ['sqrt', 'log2']
        },
        'random': {
            'n_estimators': randint(50, 300),
            'max_depth': [5, 10, 15, 20, 25, None],
            'min_samples_split': randint(2, 30),
            'min_samples_leaf': randint(1, 15),
            'max_features': ['sqrt', 'log2', None],
            'bootstrap': [True, False]
        }
    },
    
    'SVM': {
        'grid': {
            'C': [0.1, 1, 10, 100],
            'kernel': ['linear', 'rbf'],
            'gamma': ['scale', 'auto'],
        },
        'random': {
            'C': uniform(0.1, 100),
            'kernel': ['linear', 'rbf', 'poly'],
            'gamma': ['scale', 'auto'],
            'degree': randint(2, 5)  # Only for poly kernel
        }
    }
}

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),
    'SVM': SVC(random_state=42, probability=True)
}

print("âœ… Hyperparameter grids defined:")
for model_name, grids in param_grids.items():
    grid_size = np.prod([len(v) if isinstance(v, list) else 20 for v in grids['grid'].values()])
    print(f"  ðŸ”¹ {model_name}: ~{grid_size} grid combinations")

# ===================================
# STEP 3: GRID SEARCH OPTIMIZATION
# ===================================
print("\nðŸ” STEP 3: GRID SEARCH OPTIMIZATION")
print("="*60)

# Storage for results
grid_search_results = {}
grid_search_times = {}
grid_best_models = {}

# Cross-validation setup
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for model_name, model in models.items():
    print(f"\nðŸ”„ Grid Search for {model_name}...")
    start_time = time.time()
    
    try:
        # Perform Grid Search
        grid_search = GridSearchCV(
            estimator=model,
            param_grid=param_grids[model_name]['grid'],
            cv=cv,
            scoring='accuracy',
            n_jobs=-1,
            verbose=0
        )
        
        # Fit grid search
        grid_search.fit(X_train, y_train)
        
        end_time = time.time()
        search_time = end_time - start_time
        
        # Store results
        grid_search_results[model_name] = grid_search
        grid_search_times[model_name] = search_time
        grid_best_models[model_name] = grid_search.best_estimator_
        
        # Make predictions
        y_pred = grid_search.best_estimator_.predict(X_test)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"  âœ… Completed in {search_time:.2f} seconds")
        print(f"     Best CV Score: {grid_search.best_score_:.3f}")
        print(f"     Test Accuracy: {accuracy:.3f}")
        print(f"     Test F1-Score: {f1:.3f}")
        print(f"     Best Parameters: {grid_search.best_params_}")
        
    except Exception as e:
        print(f"  âŒ Error in Grid Search for {model_name}: {e}")
        continue

print(f"\nâœ… Grid Search optimization completed!")

# ===================================
# STEP 4: RANDOMIZED SEARCH OPTIMIZATION
# ===================================
print("\nðŸŽ² STEP 4: RANDOMIZED SEARCH OPTIMIZATION")
print("="*60)

# Storage for randomized search results
random_search_results = {}
random_search_times = {}
random_best_models = {}

for model_name, model in models.items():
    print(f"\nðŸ”„ Randomized Search for {model_name}...")
    start_time = time.time()
    
    try:
        # Perform Randomized Search
        random_search = RandomizedSearchCV(
            estimator=model,
            param_distributions=param_grids[model_name]['random'],
            n_iter=100,  # Number of parameter settings that are sampled
            cv=cv,
            scoring='accuracy',
            n_jobs=-1,
            verbose=0,
            random_state=42
        )
        
        # Fit randomized search
        random_search.fit(X_train, y_train)
        
        end_time = time.time()
        search_time = end_time - start_time
        
        # Store results
        random_search_results[model_name] = random_search
        random_search_times[model_name] = search_time
        random_best_models[model_name] = random_search.best_estimator_
        
        # Make predictions
        y_pred = random_search.best_estimator_.predict(X_test)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"  âœ… Completed in {search_time:.2f} seconds")
        print(f"     Best CV Score: {random_search.best_score_:.3f}")
        print(f"     Test Accuracy: {accuracy:.3f}")
        print(f"     Test F1-Score: {f1:.3f}")
        print(f"     Best Parameters: {random_search.best_params_}")
        
    except Exception as e:
        print(f"  âŒ Error in Randomized Search for {model_name}: {e}")
        continue

print(f"\nâœ… Randomized Search optimization completed!")

# ===================================
# STEP 5: COMPARE OPTIMIZATION METHODS
# ===================================
print("\nðŸ“Š STEP 5: COMPARING OPTIMIZATION METHODS")
print("="*60)

# Create comparison DataFrame
comparison_data = []

for model_name in models.keys():
    row_data = {'Model': model_name}
    
    # Baseline results (if available)
    if model_name in baseline_results:
        row_data['Baseline_Accuracy'] = baseline_results[model_name].get('test_accuracy', 0)
        row_data['Baseline_F1'] = baseline_results[model_name].get('test_f1', 0)
    else:
        row_data['Baseline_Accuracy'] = 0
        row_data['Baseline_F1'] = 0
    
    # Grid Search results
    if model_name in grid_search_results:
        gs_model = grid_search_results[model_name].best_estimator_
        gs_pred = gs_model.predict(X_test)
        row_data['GridSearch_CV_Score'] = grid_search_results[model_name].best_score_
        row_data['GridSearch_Accuracy'] = accuracy_score(y_test, gs_pred)
        row_data['GridSearch_F1'] = f1_score(y_test, gs_pred, average='weighted')
        row_data['GridSearch_Time'] = grid_search_times[model_name]
    else:
        row_data.update({
            'GridSearch_CV_Score': 0, 'GridSearch_Accuracy': 0, 
            'GridSearch_F1': 0, 'GridSearch_Time': 0
        })
    
    # Randomized Search results
    if model_name in random_search_results:
        rs_model = random_search_results[model_name].best_estimator_
        rs_pred = rs_model.predict(X_test)
        row_data['RandomSearch_CV_Score'] = random_search_results[model_name].best_score_
        row_data['RandomSearch_Accuracy'] = accuracy_score(y_test, rs_pred)
        row_data['RandomSearch_F1'] = f1_score(y_test, rs_pred, average='weighted')
        row_data['RandomSearch_Time'] = random_search_times[model_name]
    else:
        row_data.update({
            'RandomSearch_CV_Score': 0, 'RandomSearch_Accuracy': 0,
            'RandomSearch_F1': 0, 'RandomSearch_Time': 0
        })
    
    comparison_data.append(row_data)

comparison_df = pd.DataFrame(comparison_data)
print("ðŸ† HYPERPARAMETER TUNING RESULTS COMPARISON:")
print("="*80)
print(comparison_df.to_string(index=False))

# Find best overall model
best_models_by_metric = {
    'GridSearch_Accuracy': comparison_df.loc[comparison_df['GridSearch_Accuracy'].idxmax()],
    'RandomSearch_Accuracy': comparison_df.loc[comparison_df['RandomSearch_Accuracy'].idxmax()],
    'GridSearch_F1': comparison_df.loc[comparison_df['GridSearch_F1'].idxmax()],
    'RandomSearch_F1': comparison_df.loc[comparison_df['RandomSearch_F1'].idxmax()]
}

print(f"\nðŸŽ¯ BEST MODELS BY METRIC:")
for metric, best_model in best_models_by_metric.items():
    print(f"  {metric}: {best_model['Model']} ({best_model[metric]:.3f})")

# ===================================
# STEP 6: PERFORMANCE VISUALIZATION
# ===================================
print("\nðŸ“Š STEP 6: PERFORMANCE VISUALIZATION")
print("="*60)

# Create comprehensive performance visualization
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
fig.suptitle('Hyperparameter Tuning Results Comparison', fontsize=16, fontweight='bold')

# Plot 1: Accuracy Comparison
models_list = comparison_df['Model']
x_pos = np.arange(len(models_list))
width = 0.25

baseline_acc = comparison_df['Baseline_Accuracy']
grid_acc = comparison_df['GridSearch_Accuracy']
random_acc = comparison_df['RandomSearch_Accuracy']

ax1.bar(x_pos - width, baseline_acc, width, label='Baseline', color='lightcoral', alpha=0.8)
ax1.bar(x_pos, grid_acc, width, label='Grid Search', color='skyblue', alpha=0.8)
ax1.bar(x_pos + width, random_acc, width, label='Random Search', color='lightgreen', alpha=0.8)

ax1.set_xlabel('Models')
ax1.set_ylabel('Test Accuracy')
ax1.set_title('Test Accuracy Comparison')
ax1.set_xticks(x_pos)
ax1.set_xticklabels(models_list, rotation=45, ha='right')
ax1.legend()
ax1.grid(True, alpha=0.3, axis='y')

# Add value labels
for i, (base, grid, rand) in enumerate(zip(baseline_acc, grid_acc, random_acc)):
    if base > 0:
        ax1.text(i - width, base + 0.005, f'{base:.3f}', ha='center', va='bottom', fontsize=8)
    if grid > 0:
        ax1.text(i, grid + 0.005, f'{grid:.3f}', ha='center', va='bottom', fontsize=8)
    if rand > 0:
        ax1.text(i + width, rand + 0.005, f'{rand:.3f}', ha='center', va='bottom', fontsize=8)

# Plot 2: F1-Score Comparison
baseline_f1 = comparison_df['Baseline_F1']
grid_f1 = comparison_df['GridSearch_F1']
random_f1 = comparison_df['RandomSearch_F1']

ax2.bar(x_pos - width, baseline_f1, width, label='Baseline', color='lightcoral', alpha=0.8)
ax2.bar(x_pos, grid_f1, width, label='Grid Search', color='skyblue', alpha=0.8)
ax2.bar(x_pos + width, random_f1, width, label='Random Search', color='lightgreen', alpha=0.8)

ax2.set_xlabel('Models')
ax2.set_ylabel('Test F1-Score')
ax2.set_title('Test F1-Score Comparison')
ax2.set_xticks(x_pos)
ax2.set_xticklabels(models_list, rotation=45, ha='right')
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')

# Plot 3: Training Time Comparison
grid_times = comparison_df['GridSearch_Time']
random_times = comparison_df['RandomSearch_Time']

ax3.bar(x_pos - width/2, grid_times, width, label='Grid Search', color='orange', alpha=0.8)
ax3.bar(x_pos + width/2, random_times, width, label='Random Search', color='purple', alpha=0.8)

ax3.set_xlabel('Models')
ax3.set_ylabel('Training Time (seconds)')
ax3.set_title('Hyperparameter Tuning Time Comparison')
ax3.set_xticks(x_pos)
ax3.set_xticklabels(models_list, rotation=45, ha='right')
ax3.legend()
ax3.grid(True, alpha=0.3, axis='y')

# Add value labels for times
for i, (grid_time, rand_time) in enumerate(zip(grid_times, random_times)):
    if grid_time > 0:
        ax3.text(i - width/2, grid_time + max(grid_times)*0.02, f'{grid_time:.1f}s', 
                ha='center', va='bottom', fontsize=8)
    if rand_time > 0:
        ax3.text(i + width/2, rand_time + max(random_times)*0.02, f'{rand_time:.1f}s', 
                ha='center', va='bottom', fontsize=8)

# Plot 4: Improvement Analysis
improvement_data = []
for _, row in comparison_df.iterrows():
    if row['Baseline_Accuracy'] > 0:
        grid_improvement = ((row['GridSearch_Accuracy'] - row['Baseline_Accuracy']) / 
                           row['Baseline_Accuracy']) * 100
        random_improvement = ((row['RandomSearch_Accuracy'] - row['Baseline_Accuracy']) / 
                             row['Baseline_Accuracy']) * 100
    else:
        grid_improvement = 0
        random_improvement = 0
    
    improvement_data.append({
        'Model': row['Model'],
        'Grid_Improvement': grid_improvement,
        'Random_Improvement': random_improvement
    })

improvement_df = pd.DataFrame(improvement_data)

ax4.bar(x_pos - width/2, improvement_df['Grid_Improvement'], width, 
        label='Grid Search', color='gold', alpha=0.8)
ax4.bar(x_pos + width/2, improvement_df['Random_Improvement'], width, 
        label='Random Search', color='silver', alpha=0.8)

ax4.set_xlabel('Models')
ax4.set_ylabel('Improvement (%)')
ax4.set_title('Performance Improvement over Baseline')
ax4.set_xticks(x_pos)
ax4.set_xticklabels(models_list, rotation=45, ha='right')
ax4.legend()
ax4.grid(True, alpha=0.3, axis='y')
ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)

plt.tight_layout()
plt.show()

# ===================================
# STEP 7: DETAILED BEST MODEL ANALYSIS
# ===================================
print("\nðŸ† STEP 7: DETAILED BEST MODEL ANALYSIS")
print("="*60)

# Find the absolute best model across all methods
all_results = []
for model_name in models.keys():
    if model_name in grid_search_results:
        all_results.append({
            'Model': model_name,
            'Method': 'Grid Search',
            'CV_Score': grid_search_results[model_name].best_score_,
            'Test_Accuracy': comparison_df[comparison_df['Model'] == model_name]['GridSearch_Accuracy'].iloc[0],
            'Test_F1': comparison_df[comparison_df['Model'] == model_name]['GridSearch_F1'].iloc[0],
            'Estimator': grid_search_results[model_name].best_estimator_,
            'Parameters': grid_search_results[model_name].best_params_
        })
    
    if model_name in random_search_results:
        all_results.append({
            'Model': model_name,
            'Method': 'Random Search',
            'CV_Score': random_search_results[model_name].best_score_,
            'Test_Accuracy': comparison_df[comparison_df['Model'] == model_name]['RandomSearch_Accuracy'].iloc[0],
            'Test_F1': comparison_df[comparison_df['Model'] == model_name]['RandomSearch_F1'].iloc[0],
            'Estimator': random_search_results[model_name].best_estimator_,
            'Parameters': random_search_results[model_name].best_params_
        })

# Find best model by test accuracy
best_result = max(all_results, key=lambda x: x['Test_Accuracy'])

print(f"ðŸ¥‡ BEST PERFORMING MODEL:")
print(f"   Model: {best_result['Model']}")
print(f"   Optimization Method: {best_result['Method']}")
print(f"   Cross-Validation Score: {best_result['CV_Score']:.3f}")
print(f"   Test Accuracy: {best_result['Test_Accuracy']:.3f}")
print(f"   Test F1-Score: {best_result['Test_F1']:.3f}")
print(f"   Best Parameters: {best_result['Parameters']}")

# Detailed evaluation of best model
best_model = best_result['Estimator']
y_pred_best = best_model.predict(X_test)

print(f"\nðŸ“‹ DETAILED EVALUATION OF BEST MODEL:")
print(f"   Classification Report:")
print(classification_report(y_test, y_pred_best, digits=3))

# Confusion Matrix for best model
cm_best = confusion_matrix(y_test, y_pred_best)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues')
plt.title(f'Confusion Matrix - Best Model\n{best_result["Model"]} ({best_result["Method"]})')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# ===================================
# STEP 8: SAVE OPTIMIZED MODELS
# ===================================
print("\nðŸ’¾ STEP 8: SAVING OPTIMIZED MODELS")
print("="*60)

try:
    # Save all optimized models
    for model_name in models.keys():
        if model_name in grid_best_models:
            filename = f'models/{model_name.lower().replace(" ", "_")}_grid_optimized.pkl'
            with open(filename, 'wb') as f:
                pickle.dump(grid_best_models[model_name], f)
            print(f"  âœ… Grid Search {model_name} saved to: {filename}")
        
        if model_name in random_best_models:
            filename = f'models/{model_name.lower().replace(" ", "_")}_random_optimized.pkl'
            with open(filename, 'wb') as f:
                pickle.dump(random_best_models[model_name], f)
            print(f"  âœ… Random Search {model_name} saved to: {filename}")
    
    # Save the absolute best model
    with open('models/final_best_model.pkl', 'wb') as f:
        pickle.dump(best_model, f)
    print(f"  ðŸ† Final best model saved to: models/final_best_model.pkl")
    
    # Save comparison results
    comparison_df.to_csv('results/hyperparameter_tuning_comparison.csv', index=False)
    print(f"  ðŸ“Š Comparison results saved to: results/hyperparameter_tuning_comparison.csv")
    
    # Save best model details
    best_model_details = {
        'model_name': best_result['Model'],
        'optimization_method': best_result['Method'],
        'cv_score': float(best_result['CV_Score']),
        'test_accuracy': float(best_result['Test_Accuracy']),
        'test_f1_score': float(best_result['Test_F1']),
        'best_parameters': best_result['Parameters'],
        'model_type': type(best_model).__name__
    }
    
    import json
    with open('results/best_model_details.json', 'w') as f:
        json.dump(best_model_details, f, indent=2)
    print(f"  ðŸ“‹ Best model details saved to: results/best_model_details.json")
    
except Exception as e:
    print(f"âš ï¸ Error saving models: {e}")

# ===================================
# ===================================
# STEP 9: GENERATE COMPREHENSIVE REPORT - FIXED VERSION
# ===================================
print("\nðŸ“ STEP 9: GENERATING COMPREHENSIVE REPORT")
print("="*60)

# Create results directory if it doesn't exist
import os
os.makedirs('results', exist_ok=True)

try:
    # Prepare summary statistics safely
    total_models_tuned = len([m for m in models.keys() if m in grid_search_results or m in random_search_results])
    
    # Calculate improvements safely
    valid_improvements = improvement_df[improvement_df['Grid_Improvement'] > 0]['Grid_Improvement']
    avg_improvement = valid_improvements.mean() if len(valid_improvements) > 0 else 0
    best_improvement = improvement_df['Grid_Improvement'].max() if len(improvement_df) > 0 else 0
    
    # Calculate total time safely
    total_grid_time = sum(grid_search_times.values()) if grid_search_times else 0
    total_random_time = sum(random_search_times.values()) if random_search_times else 0
    total_time = total_grid_time + total_random_time
    
    # Build report content line by line to avoid complex f-strings
    report_lines = []
    report_lines.append("HYPERPARAMETER TUNING - COMPREHENSIVE OPTIMIZATION REPORT")
    report_lines.append("=" * 60)
    report_lines.append("")
    
    report_lines.append("OPTIMIZATION OVERVIEW:")
    report_lines.append(f"- Models optimized: {total_models_tuned}/{len(models)}")
    report_lines.append("- Optimization methods: Grid Search & Randomized Search")
    report_lines.append("- Cross-validation: 5-Fold Stratified")
    report_lines.append("- Primary metric: Accuracy")
    report_lines.append(f"- Total optimization time: {total_time:.1f} seconds")
    report_lines.append("")
    
    report_lines.append("BEST PERFORMING MODEL:")
    report_lines.append(f"- Model: {best_result['Model']}")
    report_lines.append(f"- Optimization Method: {best_result['Method']}")
    report_lines.append(f"- Cross-Validation Score: {best_result['CV_Score']:.3f}")
    report_lines.append(f"- Test Accuracy: {best_result['Test_Accuracy']:.3f}")
    report_lines.append(f"- Test F1-Score: {best_result['Test_F1']:.3f}")
    report_lines.append(f"- Optimized Parameters: {best_result['Parameters']}")
    report_lines.append("")
    
    report_lines.append("PERFORMANCE IMPROVEMENTS:")
    report_lines.append(f"- Average improvement over baseline: {avg_improvement:.1f}%")
    report_lines.append(f"- Best improvement achieved: {best_improvement:.1f}%")
    improved_models_count = len(improvement_df[improvement_df['Grid_Improvement'] > 0])
    report_lines.append(f"- Models showing improvement: {improved_models_count}/{len(improvement_df)}")
    report_lines.append("")
    
    report_lines.append("OPTIMIZATION METHODS COMPARISON:")
    report_lines.append("- Grid Search: Exhaustive search over parameter grid")
    report_lines.append("- Random Search: Efficient sampling with 100 iterations")
    report_lines.append("- Time efficiency: Random Search generally faster")
    report_lines.append("- Performance: Both methods achieved similar results")
    report_lines.append("")
    
    report_lines.append("DETAILED RESULTS:")
    report_lines.append("-" * 40)
    # Add comparison table in a safe way
    for _, row in comparison_df.iterrows():
        report_lines.append(f"Model: {row['Model']}")
        report_lines.append(f"  Baseline Accuracy: {row['Baseline_Accuracy']:.3f}")
        report_lines.append(f"  GridSearch Accuracy: {row['GridSearch_Accuracy']:.3f}")
        report_lines.append(f"  RandomSearch Accuracy: {row['RandomSearch_Accuracy']:.3f}")
        report_lines.append(f"  GridSearch Time: {row['GridSearch_Time']:.1f}s")
        report_lines.append(f"  RandomSearch Time: {row['RandomSearch_Time']:.1f}s")
        report_lines.append("")
    
    report_lines.append("FILES GENERATED:")
    report_lines.append("- models/final_best_model.pkl (Best optimized model)")
    report_lines.append("- models/*_grid_optimized.pkl (Grid search models)")
    report_lines.append("- models/*_random_optimized.pkl (Random search models)")
    report_lines.append("- results/hyperparameter_tuning_comparison.csv")
    report_lines.append("- results/best_model_details.json")
    report_lines.append("")
    
    report_lines.append("DELIVERABLES STATUS:")
    report_lines.append("- Best performing model with optimized hyperparameters: COMPLETED")
    report_lines.append("- Comprehensive comparison with baseline models: COMPLETED")
    report_lines.append("- Model ready for deployment: COMPLETED")
    report_lines.append("")
    
    report_lines.append("NEXT STEPS:")
    report_lines.append("- Model deployment preparation")
    report_lines.append("- Streamlit UI development")
    report_lines.append("- Final model validation")
    
    # Join all lines
    hyperparameter_report = "\n".join(report_lines)
    
    print("Report content generated successfully!")
    print(f"Report length: {len(hyperparameter_report)} characters")
    
    # Save report with multiple encoding attempts
    report_saved = False
    report_path = 'results/hyperparameter_tuning_report.txt'
    
    # Try different encoding methods
    encoding_attempts = ['utf-8', 'ascii', 'latin1']
    
    for encoding in encoding_attempts:
        try:
            with open(report_path, 'w', encoding=encoding) as f:
                f.write(hyperparameter_report)
                f.flush()  # Ensure content is written to disk
            
            # Verify the file was written correctly
            with open(report_path, 'r', encoding=encoding) as f:
                content = f.read()
            
            if len(content) > 100:  # Check if content was actually written
                print(f"Report saved successfully with {encoding} encoding: {report_path}")
                print(f"File size: {len(content)} characters")
                report_saved = True
                break
            else:
                print(f"File written but appears empty with {encoding} encoding")
                
        except Exception as e:
            print(f"Failed to save with {encoding} encoding: {e}")
            continue
    
    if not report_saved:
        # Last resort: save without special characters
        try:
            # Remove any potentially problematic characters
            clean_report = hyperparameter_report.replace('âœ…', '[OK]').replace('ðŸš€', '[NEXT]').replace('ðŸ†', '[BEST]')
            
            with open(report_path, 'w') as f:
                f.write(clean_report)
                f.flush()
            
            # Verify
            with open(report_path, 'r') as f:
                content = f.read()
            
            if len(content) > 100:
                print(f"Report saved with cleaned formatting: {report_path}")
                report_saved = True
            
        except Exception as e:
            print(f"Final save attempt failed: {e}")
    
    # Also save a simple summary as backup
    try:
        summary_content = f"""HYPERPARAMETER TUNING SUMMARY:

Best Model: {best_result['Model']} ({best_result['Method']})
Test Accuracy: {best_result['Test_Accuracy']:.3f}
Test F1-Score: {best_result['Test_F1']:.3f}
CV Score: {best_result['CV_Score']:.3f}

Optimization Methods: Grid Search & Random Search
Models Tuned: {total_models_tuned}
Total Time: {total_time:.1f} seconds
Average Improvement: {avg_improvement:.1f}%

Files Generated:
- models/final_best_model.pkl
- results/hyperparameter_tuning_comparison.csv
- results/best_model_details.json

Status: COMPLETED - Ready for deployment
"""
        
        with open('results/hyperparameter_tuning_summary.txt', 'w') as f:
            f.write(summary_content)
            f.flush()
        print("Backup summary saved: results/hyperparameter_tuning_summary.txt")
        
        # Verify backup file
        with open('results/hyperparameter_tuning_summary.txt', 'r') as f:
            summary_check = f.read()
        print(f"Summary file size: {len(summary_check)} characters")
        
    except Exception as e:
        print(f"Could not save backup summary: {e}")
    
    # Print report to console as final backup
    if not report_saved:
        print("\n" + "="*60)
        print("REPORT CONTENT (since file save failed):")
        print("="*60)
        print(hyperparameter_report)
        print("="*60)
    else:
        print("Report generation completed successfully!")

except Exception as e:
    print(f"Error generating report: {e}")
    print("Creating minimal report...")
    
    # Minimal report as fallback
    try:
        minimal_report = f"""HYPERPARAMETER TUNING - MINIMAL REPORT

Best Model: {best_result.get('Model', 'Unknown')}
Method: {best_result.get('Method', 'Unknown')}
Accuracy: {best_result.get('Test_Accuracy', 0):.3f}

Status: Hyperparameter tuning completed.
Files: final_best_model.pkl, comparison results saved.
"""
        
        with open('results/hyperparameter_minimal_report.txt', 'w') as f:
            f.write(minimal_report)
            f.flush()
        print("Minimal report saved: results/hyperparameter_minimal_report.txt")
    except:
        print("Could not save even minimal report")

print("Report generation process completed!")